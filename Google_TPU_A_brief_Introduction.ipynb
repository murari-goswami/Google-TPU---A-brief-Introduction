{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Google TPU: A brief Introduction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/Google-TPU---A-brief-Introduction/blob/master/Google_TPU_A_brief_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Ku6FDZKuyNu9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##What is a Tensor Processing Unit?\n",
        "\n",
        "A tensor processing unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning.\n",
        "\n",
        "![alt text](https://theusbport.com/wp-content/uploads/2017/05/Google-Tensor-Processing-Unit-Machine-Learning-696x391.jpg)\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/3dcfab715d93c1d08ad7844d5cb13d48a610dafd/68747470733a2f2f692e696d6775722e636f6d2f644d7833496c772e706e67)\n",
        "\n",
        "\n",
        "\n",
        "They've been using TPUs in their data centers since 2015\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "They designed them specifically for machine learning applications\n",
        "They use them for Google Translate, Photos, Search Assistant, Gmail, Cloud, etc.\n",
        "\n",
        "\n",
        "##Why did they make their own chip?\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/284dfd3c5aead96a3f388d95fd0b287488575218/68747470733a2f2f692e696d6775722e636f6d2f4e31654f79396d2e706e67)\n",
        "\n",
        "\n",
        "1)  Neural Networks in particular almost always outperform other machine learning models if given enough data & compute.\n",
        "\n",
        "2) Neural networks require a lot of compute! It's called deep learning.\n"
      ]
    },
    {
      "metadata": {
        "id": "aQpXiIRh8oHP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://farm2.staticflickr.com/1640/25046013104_68059057ab_b.jpg)\n",
        "\n",
        "\n",
        "### RISC, CISC and the TPU instruction set\n",
        "\n",
        "Programmability was another important design goal for the TPU. The TPU is not designed to run just one type of neural network model. Instead, it's designed to be flexible enough to accelerate the computations needed to run many different kinds of neural network models.\n",
        "Most modern CPUs are heavily influenced by the Reduced Instruction Set Computer (RISC) design style. With RISC, the focus is to define simple instructions (e.g., load, store, add and multiply) that are commonly used by the majority of applications and then to execute those instructions as fast as possible.  Complex Instruction Set Computer (CISC) style is used  as the basis of the TPU instruction set. A CISC design focuses on implementing high-level instructions that run more complex tasks (such as calculating multiply-and-add many times) with each instruction. Let's take a look at the block diagram of the TPU.\n",
        "\n",
        "\n",
        "![alt text](https://storage.googleapis.com/gweb-cloudblog-publish/images/tpu-15dly1.max-500x500.PNG)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The TPU includes the following computational resources:\n",
        "\n",
        "**Matrix Multiplier Unit (MXU)**: 65,536 8-bit multiply-and-add units for matrix operations.\n",
        "\n",
        "**Unified Buffer (UB**): 24MB of SRAM that work as registers.\n",
        "\n",
        "**Activation Unit (AU**): Hardwired activation functions.\n",
        "\n",
        "\n",
        "To control how the MXU, UB and AU proceed with operations,  there are a dozen high-level instructions specifically designed for neural network inference. Five of these operations are highlighted below\n",
        "\n",
        "\n",
        "![alt text](https://2.bp.blogspot.com/-IUHRodkj30M/WtMnFYyDd2I/AAAAAAAABHU/jT2XjxBvuQE2VDebboe_TpLn01VQHzzbgCLcBGAs/s1600/basic_TPU_ISA.JPG)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "toVWvD1V94JW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This instruction set focuses on the major mathematical operations required for neural network inference : execute a matrix multiply between input data and weights and apply an activation function."
      ]
    },
    {
      "metadata": {
        "id": "dtEBQhuO-UB_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neural network models consist of matrix multiplies of various sizes — that’s what forms a fully connected layer, or in a CNN, it tends to be smaller matrix multiplies. This architecture is about doing those things — when you’ve accumulated all the partial sums and are outputting from the accumulators, everything goes through this activation pipeline. The nonlinearity is what makes it a neural network even if it’s mostly linear algebra.(from First in-depth look at Google's TPU architecture, the Next Platform).\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/10d16ad95a3500f9b1d52fb6b9e8640127bc5e83/68747470733a2f2f696d6167652e736c696465736861726563646e2e636f6d2f323031357363616c61776f726c642d3135303932373132333330392d6c7661312d617070363839322f39352f6e657572616c2d6e6574776f726b2d61732d612d66756e6374696f6e2d31322d3633382e6a70673f63623d31343433333537323936)\n",
        "\n",
        "\n",
        "\n",
        "Neural networks are just a series of matrix operations applied to input data\n",
        "And if theres a lot of data to input, thats a lot of matrix operations to compute\n",
        "Like a lot. Giant, giant matrices full of numbers all being multiplied in parallel -Most of the math is just 'multiply a bunch of numbers, and add the results'\n",
        "We can connect these two together in a single operation called multiply-accumulate (MAC).\n",
        "And if we don’t need to do anything else, we can multiply-accumulate really, really fast.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7eRo7f7ycPG0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "##The TPU \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/7eb65d2554be4546659722748b60bd553bbe663a/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f4463777736634f55514141674b59622e6a7067)\n",
        "\n",
        "\n",
        "1. Google took 15 months for the TPUv1, and that was astonishingly fast for an ASIC.\n",
        "2. ASICs are initially expensive, requiring specialized engineers and manufacturing costs that start at around a million dollars.\n",
        "3. And they are inflexible: there’s no way to change the chip once it’s finished.\n",
        "But if you know you’ll be doing one particular job in enough volume, the recurring benefits can make up for the initial drawbacks.\n",
        "4. ASICs are generally the fastest and most energy-efficient way to accomplish a task.\n",
        "    3 Generation of TPUS. Describe a TPU. Then Simple CPU VS TPU Add benchmark. Then Japanese MLP GPU vs TPU benchmark. Then Tensorflow Shakespeare.\n",
        "    \n",
        "    ![alt text](https://camo.githubusercontent.com/f0e06538887a845496389eea18327aeb259f5870/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a6457345f5a3877734d636f53343464744b4e61635a512e676966)\n",
        "    \n",
        "    \n",
        "    \n",
        "1. The data of a neural network is arranged in a matrix, a 2D vector.\n",
        "\n",
        "2. So, Google decided they needed to build a a matrix machine (The tensor processing unit or TPU)\n",
        "And they really only care about multiply-accumulate, so they prioritized that over other instructions that a processor would normally support.\n",
        "\n",
        "3. Google wanted to design a chip specifically for the matrix operations that neural networks require so that it would run them even more efficiently.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/55f8f86c45be5367c3f64e040e47525ae25778b6/68747470733a2f2f636c6f75642e676f6f676c652e636f6d2f7470752f646f63732f696d616765732f7470752d2d7379732d61726368332e706e67)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. TPU hardware is comprised of four independent chips.\n",
        "2. The following block diagram describes the components of a single chip.\n",
        "3. Each chip consists of two compute cores called Tensor Cores.\n",
        "4. A Tensor Core consists of scalar, vector and matrix units (MXU).\n",
        "4. In addition, 8 GB of on-chip memory (HBM) is associated with each Tensor Core.\n",
        "6. The bulk of the compute horsepower in a Cloud TPU is provided by the MXU.\n",
        "7. Each MXU is capable of performing 16K multiply-accumulate operations in each cycle.\n",
        "8. While the MXU's inputs and outputs are 32-bit floating point values, the MXU performs multiplies at reduced bfloat16 precision.\n",
        "9. Bfloat16 is a 16-bit floating point representation that provides better training and model accuracy than the IEEE half-precision representation. -From a software perspective, each of the 8 cores on a Cloud TPU can execute user computations (XLA ops) independently.\n",
        "10. High-bandwidth interconnects allow the chips to communicate directly with each other.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/ead5b246a0381847840b72895484466860f17c1e/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3630302f312a64374c6734635964514f326b7874396e536d796a2d672e706e67)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. XLA is an experimental JIT compiler for the backend of Tensorflow.\n",
        "2. It turns your TF graph into linear algebra, and it has backends of its own to run on CPUs, GPUs, or TPUs\n",
        "proprietary"
      ]
    },
    {
      "metadata": {
        "id": "NWZg4C3__XXH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://image.slidesharecdn.com/tpu-170424084633/95/tensor-processing-unit-tpu-11-638.jpg?cb=1493023791)\n",
        "\n",
        "\n",
        "Two matrices can be multiplied when the number of columns in one is the same as the number of rows in the other; otherwise, they are incompatible. The idea behind matrix units is to look at this fact in isolation: a matrix unit is a matrix with dimensions, but with the entries scooped out. More on Systolic array later.\n",
        "\n",
        "\n",
        "##Parallel Processing on Matrix Multiplying Unit(MMU)\n",
        "\n",
        "Typical RISC processors provide instructions for simple calculations such as multiplying or adding numbers. These are so-called scalar processors, as they process a single operation (= scalar operation) with each instruction.\n",
        "Even though CPUs run at clock speeds in the gigahertz range, it can still take a long time to execute large matrix operations via a sequence of scalar operations. One effective and well-known way to improve the performance of such large matrix operations is through vector processing, where the same operation is performed concurrently across a large number of data elements at the same time. CPUs incorporate instruction set extensions  that express such vector operations. The streaming multiprocessors (SMs) of GPUs are effectively vector processors, with many such SMs on a single GPU die. Machines with vector processing support can process hundreds to thousands of operations in a single clock cycle.\n",
        "\n",
        "\n",
        "![alt text](https://storage.googleapis.com/gweb-cloudblog-publish/images/tpu-116i0h.max-1200x1200.PNG)\n",
        "\n",
        "\n",
        "A CPU is a scalar machine, which means it processes instructions one step at a time.\n",
        "CPUs can peform these matrix operations pretty well.\n",
        "But Moore's Law is coming to an end."
      ]
    },
    {
      "metadata": {
        "id": "nLZNPXTOAiW2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://camo.githubusercontent.com/7a45b6d40867e836ec7680777f102af6db26ccbf/68747470733a2f2f7777772e6361726573747265616d2e636f6d2f626c6f672f77702d636f6e74656e742f75706c6f6164732f323031352f30392f4353485f4350552d4750555f496c6c757374726174696f6e2e706e67)\n",
        "\n",
        "\n",
        "A CPU is composed of just a few cores with lots of cache memory that can handle a few software threads at a time\n",
        "Luckily, GPUs (Graphics Processing Unit) can perform matrix operations orders of magnitude better than CPUs.\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/12250993fcc542e7bec54f0f799b67f09fd1861f/68747470733a2f2f7777772e7265736561726368676174652e6e65742f70726f66696c652f4162656c5f50617a2f7075626c69636174696f6e2f3233313136373139312f6669677572652f666967322f41533a33303034333738323034363130353740313434383634313336363032362f436f6d70617269736f6e2d6f662d4350552d7665727375732d4750552d6172636869746563747572652e706e67)\n",
        "\n",
        "A GPU is composed of hundreds of cores that can handle thousands of threads simultaneously.\n",
        "Thats because GPUs were designed for 3d game rendering, which often involves parallel operations -The ability of a GPU with 100+ cores to process thousands of threads can accelerate some software by 100x over a CPU alone.\n",
        "What’s more, the GPU achieves this acceleration while being more power- and cost-efficient than a CPU.\n",
        "So when neural networks run on GPUs, they run much faster than on CPUs"
      ]
    },
    {
      "metadata": {
        "id": "Ii3gSlzuBPeG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](http://share.opsy.st/54176dc2ec16c-Vivante-Sept-Fig1.jpg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A GPU is a vector machine. You can give it a long list of data — a 1D vector — and run computations on the entire list at the same time.\n",
        "This way, we can perform more computations per second, but we have to perform the same computation on a vector of data in parallel.\n",
        "GPUs are general purpose chips. They don't just perform matrix operations, they can really do any kind of computation.\n",
        "GPUs are optimized for taking huge batches of data and performing the same operation over and over very quickly\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/750/1*Yx9XF4H4spE8Bm8XVY1bYA.png)"
      ]
    },
    {
      "metadata": {
        "id": "D2pSsAAq-jtY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Why Matrix Multiplication ?\n",
        "\n",
        "Google claimed their TPU is orders of magnitude better in performance and energy efficiency than CPUs and GPUs; further, it attributed the success to a domain-specific design. In the blog post AI Drives the Rise of Accelerated Computing in Data Centers, NVIDIA responded to these claims with their own performance metrics and concluded that TPUs and GPUs share the common themes of domain-specific acceleration of tensor computations. Tensors are high-dimensional data arrays used to represent layers of Deep Neural Networks. A Deep Learning task can be described as a Tensor Computation Graph:\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*0nV2oLkW4-mW_3h6joOGPg.png)\n",
        "\n",
        "A TPU is domain-specific to matrix computations in Deep Learning. A GPU has evolved from being domain-specific to 3d graphics into a general-purpose parallel computing machine. What makes GPU domain-specific to Deep Learning is its highly optimized matrix library, previously developed for HPC.\n",
        "\n",
        "When a GPU is used for Deep Learning, tensors are unfolded into 2-dimensional matrices, and matrix computations are handled by calling matrix kernels from the host CPU; matrix kernels refer to GPU programs implementing different types of matrix computations. Matrix multiplication comprises many MAC (multiply accumulate) operations. These MAC operations are the most time consuming part of Deep Learning. Even though the GPU environment allows programmers to write their own matrix kernels, they predominately use the pre-built matrix multiplication kernels as black boxes"
      ]
    },
    {
      "metadata": {
        "id": "Jm4CoVUYBu9u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Matrix Machine\n",
        "\n",
        "\n",
        "The data of a neural network is arranged in a matrix, a 2D vector. So, we’ll build a** matrix machine**. And we really only care about multiply-accumulate, so we’ll prioritize that over other instructions that a processor would normally support. We’ll devote most of our chip to the MACs that perform matrix multiplication, and mostly ignore other operations.(same picture as above.)\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1000/1*dW4_Z8wsMcoS44dtKNacZQ.gif)\n",
        "\n",
        "\n",
        "##Systolic Array\n",
        "\n",
        "\n",
        "The way to achieve that matrix performance is through a piece of architecture called a systolic array. This is the interesting bit, and it’s why a TPU is performant. A systolic array is a kind of hardware algorithm, and it describes a pattern of cells on a chip that computes matrix multiplication. “Systolic” describes how data moves in waves across the chip, like the beating of a human heart.\n",
        "\n",
        "\n",
        "\n",
        "Consider a matrix multiply operation:\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1000/1*umH-qhj3j1Z1k35uBkXM8g.png)\n",
        "\n",
        "\n",
        "\n",
        "For 2x2 inputs, each term in the output is the sum of two products. No product is reused, but the individual terms are.\n",
        "\n",
        "We’ll implement this by building a 2x2 grid. (It’s actually a grid, not just an abstraction — hardware is fun like that). Note that 2x2 is a toy example, and the full size MXU is 128x128.\n",
        "\n",
        "Let’s say AB/CD represents our activations and EF/GH our weights. For our array, we first load up the weights like so:\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1000/1*P83xXpFMjgLkAXf9i4uC9g.png)\n",
        "\n",
        "\n",
        "\n",
        "Our activations go into an input queue, which for our example will go on the left of each row.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1000/1*PyZRdGouPq26ON8G1vo7gQ.png/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Every cycle of the clock, each cell will execute the following steps, all in parallel:\n",
        "\n",
        "Multiply our weight and the activation coming in from the left. If there’s no cell on the left, take from the input queue.\n",
        "Add that product to the partial sum coming in from above. If there’s no cell above, the partial sum from above is zero.\n",
        "Pass the activation to the cell to the right. If there’s no cell on the right, throw away the activation.\n",
        "Pass the partial sum to the cell on the bottom. If there’s no cell on the bottom, collect the partial sum as an output."
      ]
    },
    {
      "metadata": {
        "id": "8m8gf35dokBj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "By these rules, you can see the activations will start on the left and move one cell to the right per cycle, and the partial sums will start on top amd move one cell down per cycle.\n",
        "\n",
        "The data flow will look like this:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1000/1*b7t8CxJJ8K61BC0NHXjfyg.png)\n",
        "\n",
        "\n",
        "\n",
        "And that’s our array! Let’s walk through the execution of the first output:\n",
        "\n",
        "###Cycle 1\n",
        "\n",
        "Top left reads A from input queue, multiplies with weight E to produce product AE.\n",
        "AE added to partial sum 0 from above, produces partial sum AE.\n",
        "Activation A passed to the cell in the top right.\n",
        "Partial sum AE passed to the cell in the bottom left.\n",
        "\n",
        "\n",
        "###Cycle 2\n",
        "\n",
        "Bottom left reads B off the input queue, multiplies with weight G to produce product BG\n",
        "BG added to partial sum AE from above, produces partial sum AE + BG\n",
        "Activation B passed to the cell in the bottom right\n",
        "Partial sum AE + BG is an output.\n",
        "And you can see that we’ve correctly computed the first term of our output matrix. Meanwhile, in cycle 2 we’re also computing CE in the top left and AF in the top right. These will propagate through the cells and by cycle 4, we’ll have produced the entire 2x2 output.\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/9a9264516c7dab7147ec2af23c1e338abe7f4a1e/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3630302f312a5f76375245573256576f56736f62394853376b4d46772e676966)\n",
        "\n",
        "It takes 3n-2 cycles to fully compute the result matrix, whereas a standard sequential solution is n³.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/750/1*IyFcYIFIaMwAn90RIJYfMw.png)\n",
        "\n",
        "\n",
        "\n",
        "We can do this because we’re running 128x128 MAC operations in parallel.\n",
        "Multipliers are usually big and expensive to implement in hardware, but the high density of systolic arrays lets Google pack 16,384 of them into the MXU.\n",
        "This translates directly into speed training and running your network.\n",
        "Weights are loaded in much the same way as activations — through the input queue.\n",
        "We just send a special control signal (red in the above diagram) to tell the array to store weights as they pass by, instead of running MAC operations.\n",
        "Weights remain in the same processing elements, so we can send an entire batch through before loading a new set, reducing overhead.\n",
        "That’s it! The rest of the chip is important and worth going over, but the core advantage of the TPU is its MXU — a systolic array matrix multiplication unit.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UYCo59yMackO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Use Cases for TPUs\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/ee5bbf231dc722a9bfab5a3ce188ce237a7e5993/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f677765622d636c6f7564626c6f672d7075626c6973682f6f726967696e616c5f696d616765732f7470752d36746c656c2e504e47)\n",
        "\n",
        "###CPUs:\n",
        "\n",
        "1. Quick prototyping that requires maximum flexibility\n",
        "2. Simple models that do not take long to train Small models with small effective batch sizes\n",
        "3. Models that are dominated by custom TensorFlow operations written in C++\n",
        "4. Models that are limited by available I/O or the networking bandwidth of the host system\n",
        "\n",
        "###GPUs:\n",
        "\n",
        "1. Models that are not written in TensorFlow or cannot be written in TensorFlow\n",
        "2. Models for which source does not exist or is too erroneous to change\n",
        "3. Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\n",
        "4. Models with TensorFlow ops that are not available on Cloud TPU (see the list of available TensorFlow ops)\n",
        "5. Medium-to-large models with larger effective batch sizes.\n",
        "\n",
        "###TPUs:\n",
        "\n",
        "1. Models dominated by matrix computations\n",
        "2. Models with no custom TensorFlow operations inside the main training loop\n",
        "3. Models that train for weeks or months\n",
        "4. Larger and very large models with very large effective batch sizes"
      ]
    },
    {
      "metadata": {
        "id": "sBqX9GZ4bVbk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Let's look at some code! -\n",
        "\n",
        "###Multilayer perceptron for image classification using the CIFAR Dataset (GPU vs TPU)"
      ]
    },
    {
      "metadata": {
        "id": "QnQHIYxTbXg7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "from tensorflow.contrib.tpu.python.tpu import keras_support\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, AveragePooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "\n",
        "def basic_mlp_module(input, units):\n",
        "    x = Dense(units)(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    return x\n",
        "\n",
        "def create_mlp_model():\n",
        "    input = Input((32*32*3,))\n",
        "    x = basic_mlp_module(input, 2048)\n",
        "    x = basic_mlp_module(x, 1024)\n",
        "    x = basic_mlp_module(x, 512)\n",
        "    x = basic_mlp_module(x, 256)\n",
        "    x = basic_mlp_module(x, 128)\n",
        "    x = basic_mlp_module(x, 64)\n",
        "    x = basic_mlp_module(x, 32)\n",
        "    x = basic_mlp_module(x, 16)\n",
        "    x = Dense(10, activation=\"softmax\")(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def main():\n",
        "    # 7 lets make this into TPU Code!!!\n",
        "    K.clear_session()\n",
        "\n",
        "    # CIFAR\n",
        "    (X_train, y_train), (_, _) = cifar10.load_data()\n",
        "    X_train = (X_train / 255.0).reshape(50000, -1)\n",
        "    y_train = to_categorical(y_train)\n",
        "\n",
        "    # Model building\n",
        "    model = create_mlp_model()\n",
        "    model.compile(tf.train.AdamOptimizer(learning_rate=1e-3), loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "    model.fit(X_train, y_train, batch_size=1024, epochs=10)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}